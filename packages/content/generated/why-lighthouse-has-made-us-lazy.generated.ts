/** THIS IS A GENERATED FILE
 * @command pnpm build
 */
const metaData = {
  code: 'var Component=(()=>{var d=Object.create;var o=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,u=Object.prototype.hasOwnProperty;var g=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),b=(n,e)=>{for(var i in e)o(n,i,{get:e[i],enumerable:!0})},r=(n,e,i,a)=>{if(e&&typeof e=="object"||typeof e=="function")for(let s of p(e))!u.call(n,s)&&s!==i&&o(n,s,{get:()=>e[s],enumerable:!(a=m(e,s))||a.enumerable});return n};var w=(n,e,i)=>(i=n!=null?d(f(n)):{},r(e||!n||!n.__esModule?o(i,"default",{value:n,enumerable:!0}):i,n)),y=n=>r(o({},"__esModule",{value:!0}),n);var c=g((I,l)=>{l.exports=_jsx_runtime});var T={};b(T,{default:()=>S,frontmatter:()=>v});var t=w(c()),v={title:"Frontend Performance: Why Lighthouse has made us lazy",published:new Date(1609900253235),stub:"The greatest trap of optimising web application performance is failing to understand the limitations of the tools we use to measure it, and the type of performance we\\u2019re trying to extract.",tags:["performance","web development","lighthouse"]};function h(n){let e=Object.assign({p:"p",em:"em",ul:"ul",li:"li",blockquote:"blockquote",a:"a"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:`The greatest trap of optimising web application performance is failing to\nunderstand the limitations of the tools we use to measure it, and the type of\nperformance we\\u2019re trying to extract. These are far less objective decisions than\nmost would like to believe and don\\u2019t naturally lead to the same outcome.`}),`\n`,(0,t.jsxs)(e.p,{children:[`Consider the performance needs a content-driven website, like a site that\ndelivers news content, a wiki, or maybe a basic ecommerce offering will require.\nMostly, they\\u2019ll feel performant if they `,(0,t.jsx)(e.em,{children:"load fast"}),`. So how can we make a site\nload fast? Well, we can;`]}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:`Send less bytes over the wire and cache where possible (scripts, stylesheets,\nassets etc), defer non-critical assets where possible and asynchronously load\nadditional content`}),`\n`,(0,t.jsx)(e.li,{children:"Serve static or server rendered pages to show content faster (without scripts)"}),`\n`,(0,t.jsx)(e.li,{children:"Minimise above the fold reflows"}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:"If executed well, the above will make a site load fast."}),`\n`,(0,t.jsx)(e.p,{children:`But some of these things come at an implicit performance tradeoff, and this\ntradeoff needs to be considered more carefully the more rich / or complex a web\napplication becomes. For a proper Single Page Application (SPA), or non-trivial\nweb application pushing load time too far starts to erode the SPA\\u2019s runtime\nperformance.`}),`\n`,(0,t.jsxs)(e.p,{children:[`Sure, bloated scripts will affect both the initial load time and runtime\noverhead of the application, but if an application requires additional deferred\nor lazily-loaded scripts to arrive before its properly functional, it\\u2019s not\nreally loaded. The loaded experience also fails to properly encompass the\ncomplex combination of interactions of an entire user session. A user might\n`,(0,t.jsx)(e.em,{children:"see"}),` content fast, but if the 99% of their time in the app thereafter feels\nsluggish, we\\u2019ll still hear the same complaints about performance.`]}),`\n`,(0,t.jsx)(e.p,{children:"Let\\u2019s consider the original two points above in the context of a SPA like Jira."}),`\n`,(0,t.jsxs)(e.blockquote,{children:[`\n`,(0,t.jsx)(e.p,{children:`Send less bytes over the wire and cache where possible (scripts, stylesheets,\nassets etc), defer non-critical assets where possible and asynchronously load\nadditional content`}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:`> Sending less bytes also means sending more later. If all of a user\\u2019s paths\nfrom a Kanban board (for example) are deferred scripts; to create an issue,\nupdate an epic, edit a sprint; every time a user moves away they\\u2019ll be hit by\nthe friction of additional loading spinners, and lost time. This is\nfundamentally performance degradation.`}),`\n`,(0,t.jsxs)(e.blockquote,{children:[`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.em,{children:`Serve static or server rendered pages to show content faster (without\nscripts)`})}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:`> SSR or even statically rendered pages still require hydration to be useful in\nthe context of a SPA. Servers need to do more processing to produce pages, and\nthe usefulness of these pages is questionable if a user is still stuck waiting\nfor more parts of a core experience to load even after the page is delivered.`}),`\n`,(0,t.jsx)(e.p,{children:`This is to say nothing of the additional complexity we add to our application\ncode by trying to allow for more asynchronous operations. And yet, we put\nconsiderable time into optimising these same things for all applications.`}),`\n`,(0,t.jsx)(e.p,{children:`Why do we do this? That\\u2019s been a burning question I\\u2019ve been pondering in my mind\nfor some time, but if I\\u2019m honest I believe it comes to down to how we measure,\nand what we measure with. The biggest issue with frontend metric-ing is the\nabsence of good proxies for things we\\u2019re used to leaning on server-side. Things\nlike CPU and Memory usage, and more user-focused things like the amount of time\nthe main thread is blocked by scripting. We can guesstimate and discern some of\nthese things using synthetic testing or by piercing the browser internals in lab\nenvironments to get some of these metrics directly, but if we want true,\n\\u2018real-user\\u2019 metrics, we\\u2019re limited.`}),`\n`,(0,t.jsx)(e.p,{children:"So invariably we measure what we can, and this gets us into trouble."}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.a,{href:"https://developer.chrome.com/docs/lighthouse/overview/",children:"Consider, Lighthouse"}),`.\nThe distilled simplicity makes it an attractive offering. TTI (Time to\nInteractive), FID (First Input Delay), FMP/TTR (First Meaningful Paint/Time to\nRender), CLS (Cumulative Layout Shift) \\u2014 together these are great heuristics for\nmeasuring a web application\\u2019s performance. And while I wouldn\\u2019t say that an\napplication could be performant and fail Lighthouse\\u2019s heuristics, it also gives\nfalse significance to what some of these metrics mean. The biggest issue with\nthem is how much they all focus on the load performance experience. And as I\\u2019ve\nalready said, this is a false idol.`]}),`\n`,(0,t.jsx)(e.p,{children:`Science struggles with ill-defined objectives. Humans love to aim for\nsimplicity. But sometimes the reality is neither. Better performance measurement\npushes most of these notions aside, using them as guides rather than gospel.\nUnfortunately it\\u2019s also more complex to instrument, more subjective, and harder\nto abstract, and when time pressed we\\u2019re lazy.`}),`\n`,(0,t.jsxs)(e.p,{children:[`This article was originally published on\n`,(0,t.jsx)(e.a,{href:"https://al-hinds.medium.com/frontend-performance-why-lighthouse-has-made-us-lazy-b7a1247db7cb",children:"medium"}),"."]})]})}function x(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(h,n)})):h(n)}var S=x;return y(T);})();\n;return Component;',
  frontmatter: {
    title: 'Frontend Performance: Why Lighthouse has made us lazy',
    published: '2021-01-06T02:30:53.235Z',
    stub: 'The greatest trap of optimising web application performance is failing to understand the limitations of the tools we use to measure it, and the type of performance we’re trying to extract.',
    tags: ['performance', 'web development', 'lighthouse'],
    modified: {
      raw: '2023-11-05T03:45:43.802Z',
      formatted: 'November 5, 2023',
    },
    created: { raw: '2021-01-06T02:30:53.235Z', formatted: 'January 6, 2021' },
    slug: 'why-lighthouse-has-made-us-lazy-2021-0-6',
  },
  errors: [],
  matter: {
    content:
      '\nThe greatest trap of optimising web application performance is failing to\nunderstand the limitations of the tools we use to measure it, and the type of\nperformance we’re trying to extract. These are far less objective decisions than\nmost would like to believe and don’t naturally lead to the same outcome.\n\nConsider the performance needs a content-driven website, like a site that\ndelivers news content, a wiki, or maybe a basic ecommerce offering will require.\nMostly, they’ll feel performant if they _load fast_. So how can we make a site\nload fast? Well, we can;\n\n- Send less bytes over the wire and cache where possible (scripts, stylesheets,\n  assets etc), defer non-critical assets where possible and asynchronously load\n  additional content\n- Serve static or server rendered pages to show content faster (without scripts)\n- Minimise above the fold reflows\n\nIf executed well, the above will make a site load fast.\n\nBut some of these things come at an implicit performance tradeoff, and this\ntradeoff needs to be considered more carefully the more rich / or complex a web\napplication becomes. For a proper Single Page Application (SPA), or non-trivial\nweb application pushing load time too far starts to erode the SPA’s runtime\nperformance.\n\nSure, bloated scripts will affect both the initial load time and runtime\noverhead of the application, but if an application requires additional deferred\nor lazily-loaded scripts to arrive before its properly functional, it’s not\nreally loaded. The loaded experience also fails to properly encompass the\ncomplex combination of interactions of an entire user session. A user might\n_see_ content fast, but if the 99% of their time in the app thereafter feels\nsluggish, we’ll still hear the same complaints about performance.\n\nLet’s consider the original two points above in the context of a SPA like Jira.\n\n> Send less bytes over the wire and cache where possible (scripts, stylesheets,\n> assets etc), defer non-critical assets where possible and asynchronously load\n> additional content\n\n\\> Sending less bytes also means sending more later. If all of a user’s paths\nfrom a Kanban board (for example) are deferred scripts; to create an issue,\nupdate an epic, edit a sprint; every time a user moves away they’ll be hit by\nthe friction of additional loading spinners, and lost time. This is\nfundamentally performance degradation.\n\n> _Serve static or server rendered pages to show content faster (without\n> scripts)_\n\n\\> SSR or even statically rendered pages still require hydration to be useful in\nthe context of a SPA. Servers need to do more processing to produce pages, and\nthe usefulness of these pages is questionable if a user is still stuck waiting\nfor more parts of a core experience to load even after the page is delivered.\n\nThis is to say nothing of the additional complexity we add to our application\ncode by trying to allow for more asynchronous operations. And yet, we put\nconsiderable time into optimising these same things for all applications.\n\nWhy do we do this? That’s been a burning question I’ve been pondering in my mind\nfor some time, but if I’m honest I believe it comes to down to how we measure,\nand what we measure with. The biggest issue with frontend metric-ing is the\nabsence of good proxies for things we’re used to leaning on server-side. Things\nlike CPU and Memory usage, and more user-focused things like the amount of time\nthe main thread is blocked by scripting. We can guesstimate and discern some of\nthese things using synthetic testing or by piercing the browser internals in lab\nenvironments to get some of these metrics directly, but if we want true,\n‘real-user’ metrics, we’re limited.\n\nSo invariably we measure what we can, and this gets us into trouble.\n\n[Consider, Lighthouse](https://developer.chrome.com/docs/lighthouse/overview/).\nThe distilled simplicity makes it an attractive offering. TTI (Time to\nInteractive), FID (First Input Delay), FMP/TTR (First Meaningful Paint/Time to\nRender), CLS (Cumulative Layout Shift) — together these are great heuristics for\nmeasuring a web application’s performance. And while I wouldn’t say that an\napplication could be performant and fail Lighthouse’s heuristics, it also gives\nfalse significance to what some of these metrics mean. The biggest issue with\nthem is how much they all focus on the load performance experience. And as I’ve\nalready said, this is a false idol.\n\nScience struggles with ill-defined objectives. Humans love to aim for\nsimplicity. But sometimes the reality is neither. Better performance measurement\npushes most of these notions aside, using them as guides rather than gospel.\nUnfortunately it’s also more complex to instrument, more subjective, and harder\nto abstract, and when time pressed we’re lazy.\n\nThis article was originally published on\n[medium](https://al-hinds.medium.com/frontend-performance-why-lighthouse-has-made-us-lazy-b7a1247db7cb).\n',
    data: {
      title: 'Frontend Performance: Why Lighthouse has made us lazy',
      published: '2021-01-06T02:30:53.235Z',
      stub: 'The greatest trap of optimising web application performance is failing to understand the limitations of the tools we use to measure it, and the type of performance we’re trying to extract.',
      tags: ['performance', 'web development', 'lighthouse'],
      modified: {
        raw: '2023-11-05T03:45:43.802Z',
        formatted: 'November 5, 2023',
      },
      created: {
        raw: '2021-01-06T02:30:53.235Z',
        formatted: 'January 6, 2021',
      },
      slug: 'why-lighthouse-has-made-us-lazy-2021-0-6',
    },
    isEmpty: false,
    excerpt: '',
  },
}
export default metaData
