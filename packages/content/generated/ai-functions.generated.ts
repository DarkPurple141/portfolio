/** THIS IS A GENERATED FILE
 * @command pnpm build
 */
const metaData = {
  code: 'var Component=(()=>{var l=Object.create;var a=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var m=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),y=(t,e)=>{for(var o in e)a(t,o,{get:e[o],enumerable:!0})},s=(t,e,o,r)=>{if(e&&typeof e=="object"||typeof e=="function")for(let i of u(e))!f.call(t,i)&&i!==o&&a(t,i,{get:()=>e[i],enumerable:!(r=p(e,i))||r.enumerable});return t};var w=(t,e,o)=>(o=t!=null?l(g(t)):{},s(e||!t||!t.__esModule?a(o,"default",{value:t,enumerable:!0}):o,t)),b=t=>s(a({},"__esModule",{value:!0}),t);var d=m((T,h)=>{h.exports=_jsx_runtime});var v={};y(v,{default:()=>x,frontmatter:()=>k});var n=w(d()),k={title:"What really changed my mind about AI - LLM functions",published:new Date(1710642653235),description:"Large language models are wildly powerful, but their weakness is their inability to properly understand their own limitations. Functions are the key to unlocking their potential.",tags:["AI"]};function c(t){let e=Object.assign({p:"p",a:"a",blockquote:"blockquote",pre:"pre",code:"code"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:`I\'ve been following the growth of LLMs (large language models) for a few years\nnow and the way their developing is nothing short of incredible. They\'re a huge\nleap forward for computing and unlock one of the biggest previous constraints on\nthe power of computers: the ability to understand complex text, parse it and\nprovide meaningful responses.`}),`\n`,(0,n.jsxs)(e.p,{children:["You can go to ",(0,n.jsx)(e.a,{href:"https://chat.openai.com/",children:"Chat GPT"}),` and have a conversation with\na computer that is so good at understanding and responding to text that it\'s\nalmost indistinguishable from a human.`]}),`\n`,(0,n.jsxs)(e.p,{children:[`What LLMs struggle with though is understanding their own limitations. They can\nbe overly confident in their responses and often provide incorrect or misleading\ninformation, and this comes from their nature as\n`,(0,n.jsx)(e.a,{href:"https://en.wikipedia.org/wiki/Stochastic",children:"stochastic"}),` models. They have varied\nresponses to the same input, and given broad enough scope can be wrong in\nbizarre and unexpected ways.`]}),`\n`,(0,n.jsx)(e.p,{children:`This partly a limitation of the technology, but it\'s also a limitation of the\nway we use them. We often treat them as if they\'re omniscient, but they don\'t\nalways perceive the meaning of the text they\'re processing in the same way we\ndo, or occasionally we\'re not quite as good as we think we are at communicating\nour own ideas.`}),`\n`,(0,n.jsxs)(e.p,{children:[`A breakthrough in my appreciation for LLMs has been in the growing use of\nfunctions. `,(0,n.jsx)(e.a,{href:"https://platform.openai.com/docs/guides/function-calling",children:"Functions"}),`\nare ways to scope the input and output of an LLM, and they\'re a way to help the\nLLM understand the context of the text it\'s processing.`]}),`\n`,(0,n.jsx)(e.p,{children:`For example, when booking a flight you need real world data based on your\nlocation, the time of year, the destination, and so on. You can\'t just ask an\nLLM "book me a flight" and expect it to understand all the context you\'re\nimplying. But if you use a function to scope the input and output of the LLM,\nyou can provide it with the context it needs to understand the request and you\ncan handle that scope with specific code to call APIs, provide real data, or\nbeing the process of booking.`}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:"What\'s the cheapest flight from London to New York in July?"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:"The function inputs might look like:"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:"language-typescript",children:`{\n  "origin": string,\n  "destination": string,\n  "date": Date\n}\n\n// parsed as\n{\n  "origin": "London",\n  "destination": "New York",\n  "date": "2024-07-01"\n}\n`})}),`\n`,(0,n.jsx)(e.p,{children:`Which might call out to a flight booking API to get the cheapest flight. The API\noutputs might look like:`}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:"language-typescript",children:`{\n  "flight": {\n    "origin": string,\n    "destination": string,\n    "date": Date,\n    "price": number\n  }\n}\n\n// returned as\n{\n  "flight": {\n    "origin": "London",\n    "destination": "New York",\n    "date": "2024-07-01",\n    "price": 300\n  }\n}\n`})}),`\n`,(0,n.jsx)(e.p,{children:`And the LLM can then understand the context of the request and provide a\nmeaningful response. eg.`}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:"The cheapest flight from London to New York in July is \\xA3300."}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:`The LLM could then go on to book the flight, or provide further information, or\nwhatever else you\'ve scoped it to do. This sort of gated, safe and deterministic\napplication of LLMs is the key to unlocking their potential and making them a\nsafe and reliable part of our future.`}),`\n`,(0,n.jsx)(e.p,{children:"It\'s very cool, and I\'m excited to see where it goes."})]})}function L(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(c,t)})):c(t)}var x=L;return b(v);})();\n;return Component;',
  frontmatter: {
    title: 'What really changed my mind about AI - LLM functions',
    published: '2024-03-17T02:30:53.235Z',
    description:
      'Large language models are wildly powerful, but their weakness is their inability to properly understand their own limitations. Functions are the key to unlocking their potential.',
    tags: ['AI'],
    modified: {
      raw: '2026-02-09T04:31:00.786Z',
      formatted: 'February 9, 2026',
    },
    created: { raw: '2024-03-17T02:30:53.235Z', formatted: 'March 17, 2024' },
    slug: 'ai-functions-2024-2-17',
  },
  errors: [],
  matter: {
    content:
      '\nI\'ve been following the growth of LLMs (large language models) for a few years\nnow and the way their developing is nothing short of incredible. They\'re a huge\nleap forward for computing and unlock one of the biggest previous constraints on\nthe power of computers: the ability to understand complex text, parse it and\nprovide meaningful responses.\n\nYou can go to [Chat GPT](https://chat.openai.com/) and have a conversation with\na computer that is so good at understanding and responding to text that it\'s\nalmost indistinguishable from a human.\n\nWhat LLMs struggle with though is understanding their own limitations. They can\nbe overly confident in their responses and often provide incorrect or misleading\ninformation, and this comes from their nature as\n[stochastic](https://en.wikipedia.org/wiki/Stochastic) models. They have varied\nresponses to the same input, and given broad enough scope can be wrong in\nbizarre and unexpected ways.\n\nThis partly a limitation of the technology, but it\'s also a limitation of the\nway we use them. We often treat them as if they\'re omniscient, but they don\'t\nalways perceive the meaning of the text they\'re processing in the same way we\ndo, or occasionally we\'re not quite as good as we think we are at communicating\nour own ideas.\n\nA breakthrough in my appreciation for LLMs has been in the growing use of\nfunctions. [Functions](https://platform.openai.com/docs/guides/function-calling)\nare ways to scope the input and output of an LLM, and they\'re a way to help the\nLLM understand the context of the text it\'s processing.\n\nFor example, when booking a flight you need real world data based on your\nlocation, the time of year, the destination, and so on. You can\'t just ask an\nLLM "book me a flight" and expect it to understand all the context you\'re\nimplying. But if you use a function to scope the input and output of the LLM,\nyou can provide it with the context it needs to understand the request and you\ncan handle that scope with specific code to call APIs, provide real data, or\nbeing the process of booking.\n\n> What\'s the cheapest flight from London to New York in July?\n\nThe function inputs might look like:\n\n```typescript\n{\n  "origin": string,\n  "destination": string,\n  "date": Date\n}\n\n// parsed as\n{\n  "origin": "London",\n  "destination": "New York",\n  "date": "2024-07-01"\n}\n```\n\nWhich might call out to a flight booking API to get the cheapest flight. The API\noutputs might look like:\n\n```typescript\n{\n  "flight": {\n    "origin": string,\n    "destination": string,\n    "date": Date,\n    "price": number\n  }\n}\n\n// returned as\n{\n  "flight": {\n    "origin": "London",\n    "destination": "New York",\n    "date": "2024-07-01",\n    "price": 300\n  }\n}\n```\n\nAnd the LLM can then understand the context of the request and provide a\nmeaningful response. eg.\n\n> The cheapest flight from London to New York in July is Â£300.\n\nThe LLM could then go on to book the flight, or provide further information, or\nwhatever else you\'ve scoped it to do. This sort of gated, safe and deterministic\napplication of LLMs is the key to unlocking their potential and making them a\nsafe and reliable part of our future.\n\nIt\'s very cool, and I\'m excited to see where it goes.\n',
    data: {
      title: 'What really changed my mind about AI - LLM functions',
      published: '2024-03-17T02:30:53.235Z',
      description:
        'Large language models are wildly powerful, but their weakness is their inability to properly understand their own limitations. Functions are the key to unlocking their potential.',
      tags: ['AI'],
      modified: {
        raw: '2026-02-09T04:31:00.786Z',
        formatted: 'February 9, 2026',
      },
      created: { raw: '2024-03-17T02:30:53.235Z', formatted: 'March 17, 2024' },
      slug: 'ai-functions-2024-2-17',
    },
    isEmpty: false,
    excerpt: '',
  },
}
export default metaData
